{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load All Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#### Write your functions and define variables\n",
    "def num_missing(x):\n",
    "return sum(x.isnull())\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "def __init__(self):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Impute missing values.\n",
    "Columns of dtype object are imputed with the most frequent value in column.\n",
    "Columns of other types are imputed with mean of column.\n",
    "\"\"\"\n",
    "def fit(self, X, y=None):\n",
    "    self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "    if X[c].dtype == np.dtype('O') \n",
    "    else X[c].mean() \n",
    "    for c in X],index=X.columns)\n",
    "return self\n",
    "def transform(self, X, y=None):\n",
    "return X.fillna(self.fill)\n",
    "#### Load Dataset\n",
    "train_filename = './datasets/av-sigmacab-train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filename = './datasets/av-sigmacab-test.csv'\n",
    "train_df = pd.read_csv(train_filename, header=0)\n",
    "test_df = pd.read_csv(test_filename, header=0)\n",
    "cols=train_df.columns\n",
    "train_df['source']='train'\n",
    "test_df['source']='test'\n",
    "data = pd.concat([train_df, test_df],ignore_index=True)\n",
    "print (train_df.shape, test_df.shape, data.shape)\n",
    "# Handling missing values\n",
    "imputer_mean = Imputer(missing_values = 'NaN', strategy =\n",
    "'mean', axis = 0)\n",
    "imputer_median = Imputer(missing_values = 'NaN', strategy\n",
    "= 'median', axis = 0)\n",
    "imputer_mode = Imputer(missing_values = 'NaN', strategy =\n",
    "'most_frequent', axis = 0)\n",
    "data[\"Life_Style_Index\"]=imputer_mean.fit_transform(data\n",
    "[[\"Life_Style_Index\"]]).ravel()\n",
    "data[\"Var1\"]=imputer_mean.fit_transform(data[[\"Var1\"]]).r\n",
    "avel()\n",
    "data[\"Customer_Since_Months\"]=imputer_median.fit_transfor\n",
    "m(data[[\"Customer_Since_Months\"]]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data)\n",
    "data = DataFrameImputer().fit_transform(X)\n",
    "print (data.apply(num_missing, axis=0))\n",
    "#Divide into test and train:\n",
    "train_df = data.loc[data['source']==\"train\"]\n",
    "test_df = data.loc[data['source']==\"test\"]\n",
    "# Drop unwanted columns\n",
    "train_df = train_df.drop(['Trip_ID','Cancellation_Last_1M\n",
    "onth','Confidence_Life_Style_Index','Gender','Life_Style_\n",
    "Index','Var1','Var2','source',],axis=1)\n",
    "#### Extract the label column\n",
    "train_target = np.ravel(np.array(train_df['Surge_Pricing_\n",
    "Type'].values))\n",
    "train_df = train_df.drop(['Surge_Pricing_Type'],axis=1)\n",
    "# Extract features\n",
    "float_columns=[]\n",
    "cat_columns=[]\n",
    "int_columns=[]\n",
    "for i in train_df.columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if train_df[i].dtype == 'float' :\n",
    "float_columns.append(i)\n",
    "elif train_df[i].dtype == 'int64':\n",
    "int_columns.append(i)\n",
    "elif train_df[i].dtype == 'object':\n",
    "cat_columns.append(i)\n",
    "train_cat_features = train_df[cat_columns]\n",
    "train_float_features = train_df[float_columns]\n",
    "train_int_features = train_df[int_columns]\n",
    "## Transformation of categorical columns\n",
    "# Label Encoding:\n",
    "#train_cat_features_ver2 = pd.get_dummies(train_cat_featu\n",
    "res, columns=['Destination_Type','Type_of_Cab'])\n",
    "train_cat_features_ver2 = train_cat_features.apply(LabelE\n",
    "ncoder().fit_transform)\n",
    "## Transformation of float columns\n",
    "# Rescale data (between 0 and 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in train_float_features.columns:\n",
    "X_temp = train_float_features[i].reshape(-1,1)\n",
    "train_float_features[i] = scaler.fit_transform(X_tem\n",
    "p)\n",
    "#### Finalize X & Y\n",
    "temp_1 = np.concatenate((train_cat_features_ver2,train_fl\n",
    "oat_features),axis=1)\n",
    "train_transformed_features = np.concatenate((temp_1,train\n",
    "_int_features),axis=1)\n",
    "train_transformed_features = pd.DataFrame(data=train_tran\n",
    "sformed_features)\n",
    "array = train_transformed_features.values\n",
    "number_of_features = len(array[0])\n",
    "X = array[:,0:number_of_features]\n",
    "Y = train_target\n",
    "# Split into training and validation set\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = cross_vali\n",
    "dation.train_test_split(X, Y, test_size=validation_size,\n",
    "random_state=seed)\n",
    "scoring = 'accuracy'\n",
    "(https://datahack.analyticsvidhya.com/contest/lordof-\n",
    "the-machines/?\n",
    "utm_source=AVblog_sidebottom)\n",
    "(https://datahack.analyticsvidhya.com/contest/datahac\n",
    "premier-league/?\n",
    "utm_source=AVblog_sidebottom)\n",
    "Subscribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 1 - Logisitic Regression\n",
    "model_logreg = LogisticRegression()\n",
    "model_logreg.fit(X_train, Y_train)\n",
    "accuracy_score(Y_validation, model_logreg.predict(X_valid\n",
    "ation))\n",
    "# Model 2 - RandomForest Classifier\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train, Y_train)\n",
    "accuracy_score(Y_validation, model_rf.predict(X_validatio\n",
    "n))\n",
    "# Model 3 - XGB Classifier\n",
    "model_xgb = XGBClassifier()\n",
    "model_xgb.fit(X_train, Y_train)\n",
    "accuracy_score(Y_validation, model_xgb.predict(X_validati\n",
    "on))\n",
    "model_logreg = LogisticRegression()\n",
    "model_logreg.fit(X, Y)\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_xgb = XGBClassifier()\n",
    "model_xgb.fit(X, Y)\n",
    "# LIME SECTION\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from __future__ import print_function\n",
    "predict_fn_logreg = lambda x: model_logreg.predict_proba\n",
    "(x).astype(float)\n",
    "predict_fn_rf = lambda x: model_rf.predict_proba(x).astyp\n",
    "e(float)\n",
    "predict_fn_xgb = lambda x: model_xgb.predict_proba(x).ast\n",
    "ype(float)\n",
    "# Line-up the feature names\n",
    "feature_names_cat = list(train_cat_features_ver2)\n",
    "feature_names_float = list(train_float_features)\n",
    "feature_names_int = list(train_int_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = sum([feature_names_cat, feature_names_flo\n",
    "at, feature_names_int], [])\n",
    "print(feature_names)\n",
    "# Create the LIME Explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_trai\n",
    "n ,feature_names = feature_names,class_names=\n",
    "['1','2','3'],\n",
    "catego\n",
    "rical_features=cat_columns,\n",
    "catego\n",
    "rical_names=feature_names_cat, kernel_width=3)\n",
    "# Pick the observation in the validation set for which ex\n",
    "planation is required\n",
    "observation_1 = 2\n",
    "# Get the explanation for Logistic Regression\n",
    "exp = explainer.explain_instance(X_validation[observation\n",
    "_1], predict_fn_logreg, num_features=6)\n",
    "exp.show_in_notebook(show_all=False)\n",
    "# Get the explanation for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(X_validation[observation\n",
    "_1], predict_fn_rf, num_features=6)\n",
    "exp.show_in_notebook(show_all=False)\n",
    "# Get the explanation for XGBoost\n",
    "exp = explainer.explain_instance(X_validation[observation\n",
    "_1], predict_fn_xgb, num_features=6)\n",
    "exp.show_in_notebook(show_all=False)\n",
    "# Look at the actual value in the validation set\n",
    "print(Y_validation[observation_1])\n",
    "# Pick the observation in the validation set for which ex\n",
    "planation is required\n",
    "observation_2 = 45\n",
    "# Get the explanation for Logistic Regression\n",
    "exp = explainer.explain_instance(X_validation[observation\n",
    "_2], predict_fn_logreg, num_features=6)\n",
    "exp.show_in_notebook(show_all=False)\n",
    "# Get the explanation for RandomForest\n",
    "exp = explainer.explain_instance(X_validation[observation\n",
    "_2], predict_fn_rf, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_all=False)\n",
    "# Get the explanation for XGBoost\n",
    "exp = explainer.explain_instance(X_validation[observation\n",
    "_2], predict_fn_xgb, num_features=6)\n",
    "exp.show_in_notebook(show_all=False)\n",
    "# Look at the actual value in the validation set\n",
    "print(Y_validation[observation_2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
